<!DOCTYPE html>
<html lang='en'>

<head>
    <base href=".">
    <link rel="icon" type="image/png" href="assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <meta name="description" content="INCAS-2025">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="INCAS2025">
    <title>INCAS 2025</title>
</head>

<body>


    <div class="container">
        <div class="top-left">
            <span class="title1">INCAS</span> <span class="title2">2025</span>
        </div>
        <table class="navigation">
            <tr>
                <td class="navigation">
                    <a class="current" title="Conference Home Page" href=".">Home</a>
                </td>
                <td class="navigation">
                    <a title="Register for the Conference" href="registration">Registration</a>
                </td>
                <td class="navigation">
                    <a title="Conference Program" href="program">Program</a>
                </td>
                <td class="navigation">
                    <a title="Venue of the Conference" href="venue">Venue</a>
                </td>
            </tr>
        </table>
    </div>


    <div class="banner">
        <img src="assets/banner.jpg" alt="INCAS-2025 Banner">
        <div class="top-center">
            ACM CoNEXT Workshop on In-Network Computing and AI for Distributed Systems (INCAS 2025)
        </div>
        <div class="bottom-right">
            December 1-4, 2025 <br> HKUST, Hong Kong
        </div>
    </div>


    <h2>Motivation of The Workshop</h2>
    <p>
        The rise of programmable network devices -- such as switches and SmartNICs -- has enabled a new computing paradigm: <strong>In-Network Computing (INC)</strong>. By performing computation directly in the data plane, INC can reduce communication overhead, accelerate distributed coordination, and improve the efficiency of system-wide operations. <strong>Distributed systems</strong> stand to benefit significantly from this paradigm shift. Many of their core functions—such as replication, consensus, scheduling, aggregation, and load balancing—are communication-heavy and latency-sensitive. In-network computing provides a compelling opportunity to rethink these mechanisms by pushing parts of their logic into the network fabric. While recent research has demonstrated the feasibility of INC for specific tasks (e.g., cache indexing, gradient aggregation, distributed counters), broader system-level integration remains challenging. Open questions span from programming abstractions and hardware constraints to performance modeling, security, and compatibility with evolving distributed runtimes. This workshop focuses on <strong>in-network computing and AI for distributed systems (INCAS)</strong>. We aim to bring together researchers and practitioners to share emerging ideas, explore new design patterns, and identify key challenges at the intersection of programmable networks and distributed system architecture.
    </p>


    <h2>Call for Papers</h2>
    <p>
        In-network computing (INC) has emerged as a powerful paradigm that integrates computation directly into the network fabric. By leveraging programmable switches, SmartNICs, and other data plane devices, INC enables unprecedented opportunities to accelerate distributed systems, data analytics, and AI workloads. As modern applications demand lower latency, higher throughput, and greater resource efficiency, INC is rapidly gaining traction in both academia and industry.
    </p>
    <p>
        INCAS 2025 aims to bring together researchers and practitioners at the intersection of networking, systems, and AI to explore how INC can reshape the architecture of distributed computing. We invite original research contributions that investigate the design, implementation, and deployment of in-network computing, especially in the context of real-world constraints such as limited hardware resources, system-level integration, and stringent performance requirements. Topics of interest include, but are not limited to: 
    </p>
    <p>
        <strong>1. INC for AI workloads</strong>
        <ul style="list-style-type: disc;">
            <li style="list-style-type: disc;">In-network acceleration of machine learning inference (e.g., CNNs, RNNs, transformers)</li>
            <li style="list-style-type: disc;">In-network acceleration for distributed model training, e.g., efficient in-network execution of collective operations</li>
            <li style="list-style-type: disc;">Explorations on deploying large vision and language models with INC</li>
        </ul>
    </p>
    <p>
        <strong>2. INC for distributed data analytics</strong>
        <ul style="list-style-type: disc;">
            <li style="list-style-type: disc;">In-network support for real-time data aggregation, transformation, and filtering</li>
            <li style="list-style-type: disc;">Acceleration of stream and batch processing pipelines</li>
            <li style="list-style-type: disc;">Integration of INC into distributed query execution and analytics engines</li>
        </ul>
    </p>
    <p>
        <strong>3. AI with INC for network management</strong>
        <ul style="list-style-type: disc;">
            <li style="list-style-type: disc;">AI-enhanced network traffic engineering using INC, intelligent traffic prediction, advanced telemetry, and AI-enabled use of INC for real-time anomaly detection and fault diagnosis</li>
            <li style="list-style-type: disc;">AI-driven network management, orchestration, and resource optimization leveraging INC, including applications of AI/ML with INC for efficient network control and automated management</li>
            <li style="list-style-type: disc;">Predictive network maintenance using INC and AI for proactive fault prediction and intelligent maintenance scheduling</li>
            <li style="list-style-type: disc;">On-path enforcement of routing, access control, and QoS policies</li>
            <li style="list-style-type: disc;">AI-assisted and INC-enabled smart cloud-edge network management and coordination</li>
            <li style="list-style-type: disc;">Programmable fault monitoring and distributed failure response mechanisms</li>
            <li style="list-style-type: disc;">Protocol enhancement via INC, e.g., redesign of end-to-end protocols to take advantage of programmable dataplanes</li>
        </ul>
    </p>
    <p>
        <strong>4. INC for data storage and caching</strong>
        <ul style="list-style-type: disc;">
            <li style="list-style-type: disc;">In-network support for key-value lookups, metadata indexing, and content-addressable storage</li>
            <li style="list-style-type: disc;">Programmable data paths for consistency enforcement, replication, and coordination in distributed storage</li>
            <li style="list-style-type: disc;">On-path caching, coherence, and eviction mechanisms for latency-sensitive workloads</li>
            <li style="list-style-type: disc;">Acceleration of storage systems and protocols (e.g., NFS, NVMe-over-Fabrics), including those leveraging RDMA-based transports</li>
            <li style="list-style-type: disc;">Offloading I/O processing and load balancing in large-scale storage clusters using programmable switches or SmartNICs</li>
        </ul>
    </p>
    <p>
        <strong>5. Protocol and system-level innovations for INC</strong>
        <ul style="list-style-type: disc;">
            <li style="list-style-type: disc;">INC-aware routing protocol design, including programmable control over path selection, route updates, and fast failoverIn-network support for congestion control, reliability, and flow scheduling</li>
            <li style="list-style-type: disc;">In-network support for transport-layer functions, such as congestion control, reliability, and flow scheduling</li>
            <li style="list-style-type: disc;">Task coordination, pipelining, and batching mechanisms within in-network systems</li>
            <li style="list-style-type: disc;">Adaptive resource allocation and scheduling for switch/NIC/FPGA-based environments</li>
        </ul>
    </p>
    <p>
        <strong>6. Tooling and development support for INC</strong>
        <ul style="list-style-type: disc;">
            <li style="list-style-type: disc;">Compilers, debuggers, and developer tools for INC platforms</li>
            <li style="list-style-type: disc;">Using foundation models (e.g., LLMs or diffusion models) to assist INC system design, and their application for intelligent network operations, management, and AI-powered assistants in INC environments.</li>
        </ul>
    </p>
    <p>
        <strong>7. Security and robustness enhancement for INC</strong>
        <ul style="list-style-type: disc;">
            <li style="list-style-type: disc;">In-network security policies and anomaly detection mechanisms</li>
            <li style="list-style-type: disc;">Fault isolation, recovery, and robustness in deployed programmable networks</li>
            <li style="list-style-type: disc;">Experiences and lessons from production-level or testbed-scale INC deployments</li>
        </ul>
    </p>
    <hr style="width: 90%; margin: 0px auto; border: 0; border-top: 3px solid #9a9090;">
    <p>
        We will follow the submission format requirements of previous CoNEXT workshops:
        <p>
            Submissions must be original, unpublished work, and not under consideration at another conference or journal. Submitted papers must be at most six (6) pages long, excluding references and appendices, in two-column 10pt ACM format. Authors of accepted submissions are expected to present and discuss their work at the workshop. <br> All submissions will be peer-reviewed, and the review process will be double-blind. Per the anonymity guidelines, please prepare your paper in a way that preserves the anonymity of the authors. No information will be shared with third parties.
        </p>
    </p>
    <hr style="width: 90%; margin: 0px auto; border: 0; border-top: 3px solid #9a9090;">
    <p>
        <strong>Expected Number of Submissions and Participants</strong>
        <p>
            We anticipate receiving around 30 paper submissions with an acceptance rate of 40-50% (select eight papers for oral presentations; the remaining as posters). Besides organizers, we expect around 50 participants.
        </p>
    </p>
    

    <h2>Important Date (All AoE Time)</h2>
    <p>
        <ul>
            <li><strong>Workshop paper abstract registration: July 8th, 2025</strong></li>
            <li><strong>Workshop paper submission: July 15th, 2025</strong></li>
            <li><strong>Notification of acceptance: August 31st, 2025 </strong></li>
            <li><strong>Camera-ready workshop papers: September 25th, 2025</strong></li>
            <li><strong>Program available online: October 12th, 2025</strong></li>
            <li><strong>List of organization details: October 12th, 2025</strong></li>
        </ul>
    </p>


    <h2>Organizers</h2>
    <p>
        <strong>Steering Co-Chairs</strong>
        <ul>
            <li> Deke Guo (National University of Defense Technology) </li>
            <li> Dezun Dong (National University of Defense Technology) </li>
            <li> Theophilus A. Benson (Carnegie Mellon University) </li>
        </ul>
    </p>

    <p>
        <strong>Organization Co-Chairs</strong>
        <ul>
            <li> Wenfei Wu (Peking University) </li>
            <li> Xiaoxi Zhang (Sun Yat-sen University) </li>
            <li> Yiran Zhang (Beijing University of Posts and Telecommunications) </li>
            <li> Qingkai Meng (Nanjing University) </li>
        </ul>
    </p>

    <p>
        <strong>Web Chair</strong>
        <ul>
            <li>Songjun Huang (Sun Yat-sen University)</li>
            <li>Zeteng Yan (Sun Yat-sen University)</li>
        </ul>
    </p>

    <p>
        <strong>Publication Chair</strong>
        <ul>
            <li>Jianqiang Zhong (Sun Yat-sen University)</li>
        </ul>
    </p>

    <p>
        <strong>Publicity Chair</strong>
        <ul>
            <li>Haoran Xu (Sun Yat-sen University)</li>
        </ul>
    </p>
    
    <p>
        <strong>Program Committee Members</strong>
        <ul>
            <li>Gianni Antichi (Politecnico di Milano and Queen Mary University of London)</li>
            <li>Wenxue Cheng (MSRA)</li>
            <li>Lin Cui (Jinan University)</li>
            <li>Jingpu Duan (Pengcheng Laboratory)</li>
            <li>Jiawei Huang (Zhongnan University)</li>
            <li>Jiayi Huang (HKUST-Guangzhou) </li>
            <li>Wanchun Jiang (Central South University) </li>
            <li>Zhuo Jiang (ByteDance)</li>
            <li>Wenxin Li (Tianjin University)</li>
            <li>Jiaxin Lin (Cornell University)</li>
            <li>Shinan Liu (HKU)</li>
            <li>Shuo Liu (Huawei)</li>
            <li>Youyou Lu (Tsinghua University)</li>
            <li>Zhixiong Niu (MSRA)</li>
            <li>Kun Qian (Alibaba)</li>
            <li>Yiming Qiu (HKU)</li>
            <li>Danfeng Shan (Xi'an Jiao Tong University)</li>
            <li>Haifeng Sun (NUS)</li>
            <li>Xiaoliang Wang (Nanjing University)</li>
            <li>Xingda Wei (Shanghai Jiao Tong University)</li>
            <li>Wenfei Wu (Peking University)</li>
            <li>Yongji Wu (UC Berkeley)</li>
            <li>Yunming Xiao (CUHK(SZ))</li>
            <li>Jiarong Xing (UC Berkeley)</li>
            <li>Hong Xu (CUHK)</li>
            <li>Lei Xue (Sun Yat-sen University)</li>
            <li>Gaoxiong Zeng (Huawei)</li>
            <li>Rongfei Zeng (Northeastern University)</li>
            <li>Menghao Zhang (Beihang University)</li>
            <li>Xiaoxi Zhang (Sun Yat-sen University)</li>
            <li>Gongming Zhao (USTC)</li>
            <li>Shizhen Zhao (Shanghai Jiao Tong University)</li>
            <li>Jiaqi Zheng (Nanjing University)</li>
            <li>Yang Zhou (UCB & UCD)</li>
            <li>Tom Barbette (UCLouvain)</li>
            <li>*<i>Potential committee members will be finalized soon</i></li>
        </ul>
    </p>

    <h2>Publicity Plan</h2>
    <p>
        To ensure strong participation and broad visibility, the <strong>INCAS 2025</strong> workshop will be promoted through multiple channels, including the official ACM CoNEXT website, academic mailing lists, and a range of social media platforms (e.g., LinkedIn, Twitter/X, WeChat, and others). We will also conduct direct outreach to leading researchers and industry professionals in relevant fields.
    </p>

    <footer>
        &copy; INCAS2025
    </footer>

</body>
</html>

>>>>>>> b9c8a70 (INCAS)
